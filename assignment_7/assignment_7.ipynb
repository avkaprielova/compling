{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "gkHbPfA8HWXD",
    "outputId": "7e079d16-2b32-4f01-cd60-480fdc3723bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n",
      "--2020-02-22 13:17:17--  http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 113157482 (108M) [application/x-gzip]\n",
      "Saving to: ‘training-parallel-nc-v13.tgz.1’\n",
      "\n",
      "training-parallel-n 100%[===================>] 107.92M  11.0MB/s    in 12s     \n",
      "\n",
      "2020-02-22 13:17:30 (9.07 MB/s) - ‘training-parallel-nc-v13.tgz.1’ saved [113157482/113157482]\n",
      "\n",
      "training-parallel-nc-v13/\n",
      "training-parallel-nc-v13/news-commentary-v13.ru-en.ru\n",
      "training-parallel-nc-v13/news-commentary-v13.cs-en.en\n",
      "training-parallel-nc-v13/news-commentary-v13.de-en.de\n",
      "training-parallel-nc-v13/news-commentary-v13.ru-en.en\n",
      "training-parallel-nc-v13/news-commentary-v13.zh-en.zh\n",
      "training-parallel-nc-v13/news-commentary-v13.zh-en.en\n",
      "training-parallel-nc-v13/news-commentary-v13.cs-en.cs\n",
      "training-parallel-nc-v13/news-commentary-v13.de-en.en\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchtext import datasets, data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import sentencepiece as spm\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math, copy, time\n",
    "%matplotlib inline\n",
    "seaborn.set_context(context=\"talk\")\n",
    "!wget http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz\n",
    "!tar zxvf training-parallel-nc-v13.tgz -C data/\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3lAU45iT8K0S",
    "outputId": "eb08e106-75fe-4c88-ee5f-77abc7c41f14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/news-commentary-v13.ru-en.en') as f:\n",
    "    with open('data/text.en', 'w') as out:\n",
    "            out.write(f.read().lower())\n",
    "        \n",
    "spm.SentencePieceTrainer.Train('--input=data/text.en --model_prefix=bpe_en --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PV01z9YS8yGq",
    "outputId": "5ae9984f-fbb5-4b05-f1f8-1ea89b29ffc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize russian\n",
    "with open('data/news-commentary-v13.ru-en.ru') as f:\n",
    "    with open('data/text.ru', 'w') as out:\n",
    "            out.write(f.read().lower())\n",
    "        \n",
    "spm.SentencePieceTrainer.Train('--input=data/text.ru --model_prefix=bpe_ru --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "66GrdWpB84XP"
   },
   "outputs": [],
   "source": [
    "tok_ru = spm.SentencePieceProcessor()\n",
    "tok_ru.load('bpe_ru.model')\n",
    "\n",
    "tok_en = spm.SentencePieceProcessor()\n",
    "tok_en.load('bpe_en.model')\n",
    "\n",
    "SRC = data.Field(\n",
    "    fix_length=50,\n",
    "    init_token='<s>',\n",
    "    eos_token='</s>',\n",
    "    lower=True,\n",
    "    tokenize = lambda x: tok_ru.encode_as_pieces(x),\n",
    "    batch_first=True,\n",
    ")\n",
    "\n",
    "TGT = data.Field(\n",
    "    fix_length=50,\n",
    "    init_token='<s>',\n",
    "    eos_token='</s>',\n",
    "    lower=True,\n",
    "    tokenize = lambda x: tok_en.encode_as_pieces(x),\n",
    "    batch_first=True,\n",
    ")\n",
    "\n",
    "fields = (('src', SRC), ('tgt', TGT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QqwTEmUY9HkK"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "68l-Ax9-851I",
    "outputId": "1ed3401b-026b-4b9d-d9f6-3ce88799eed5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "235159it [01:15, 3107.65it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('data/text.ru') as f:\n",
    "    src_snt = list(map(str.strip, f.readlines()))\n",
    "    \n",
    "with open('data/text.en') as f:\n",
    "    tgt_snt = list(map(str.strip, f.readlines()))\n",
    "    \n",
    "examples = [data.Example.fromlist(x, fields) for x in tqdm(zip(src_snt, tgt_snt))]\n",
    "test = data.Dataset(examples[-1000:], fields)\n",
    "train, valid = data.Dataset(examples[:-1000], fields).split(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "8cQTT_2U9BPp",
    "outputId": "4abc779d-311c-4b80-f2f1-3452ee5b0c7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: ▁совсем ▁недавно ▁дональд ▁к ер р , ▁заместитель ▁главы ▁национальной ▁разведки ▁сша , ▁предупредил ▁о ▁том , ▁что ▁ « шпи оны ▁не ▁отвечают ▁за ▁большую ▁часть ▁утечки ▁ин ф ормации ▁и ▁секре тов ▁наших ▁правитель ственных ▁программ .... в ▁действ ительности ▁меня ▁больше ▁беспокоит ▁тот ▁ ф акт , ▁что ▁столь ▁многие ▁возможности , ▁от ▁которых ▁мы ▁все ▁будем ▁зависеть , ▁больше ▁не ▁разрабаты ваются ▁в ▁правитель ственных ▁лабораториях ▁по ▁правитель ственным ▁контрак там » .\n",
      "tgt: ▁ j ust ▁recently , ▁donald ▁k err , ▁the ▁us ▁deputy ▁director ▁of ▁national ▁intelligence , ▁warned ▁that ▁ “ ma j or ▁losses ▁of ▁information ▁and ▁value ▁for ▁our ▁government ▁programs ▁typically ▁aren ’ t ▁from ▁spies .... in ▁fact , ▁one ▁of ▁the ▁great ▁concerns ▁i ▁have ▁is ▁that ▁so ▁much ▁of ▁the ▁new ▁capabilities ▁that ▁we ’ re ▁all ▁going ▁to ▁depend ▁on ▁aren ’ t ▁any ▁longer ▁developed ▁in ▁government ▁labs ▁under ▁government ▁contract . ”\n"
     ]
    }
   ],
   "source": [
    "print('src: ' + \" \".join(train.examples[100].src))\n",
    "print('tgt: ' + \" \".join(train.examples[100].tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YDhH_QDm9DZD",
    "outputId": "aa64fbe2-7f50-44d5-c87c-5b5d6d411973"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210743, 23416, 1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(valid), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HtHOPcH59OR_"
   },
   "outputs": [],
   "source": [
    "TGT.build_vocab(train, min_freq=5)\n",
    "SRC.build_vocab(train, min_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O4ERn-5A9QJu"
   },
   "outputs": [],
   "source": [
    "from transformer import make_model, Batch\n",
    "\n",
    "    \n",
    "class BucketIteratorWrapper(DataLoader):\n",
    "    __initialized = False\n",
    "\n",
    "    def __init__(self, iterator: data.Iterator):\n",
    "#         super(BucketIteratorWrapper,self).__init__()\n",
    "        self.batch_size = iterator.batch_size\n",
    "        self.num_workers = 1\n",
    "        self.collate_fn = None\n",
    "        self.pin_memory = False\n",
    "        self.drop_last = False\n",
    "        self.timeout = 0\n",
    "        self.worker_init_fn = None\n",
    "        self.sampler = iterator\n",
    "        self.batch_sampler = iterator\n",
    "        self.__initialized = True\n",
    "\n",
    "    def __iter__(self):\n",
    "        return map(\n",
    "            lambda batch: Batch(batch.src, batch.tgt, pad=TGT.vocab.stoi['<pad>']),\n",
    "            self.batch_sampler.__iter__()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_sampler)\n",
    "    \n",
    "class MyCriterion(nn.Module):\n",
    "    def __init__(self, pad_idx):\n",
    "        super(MyCriterion, self).__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='sum', ignore_index=pad_idx)\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        x = x.contiguous().permute(0,2,1)\n",
    "        ntokens = (target != self.pad_idx).data.sum()\n",
    "        \n",
    "        return self.criterion(x, target) / ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRXrmUjO9eJX"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "\n",
    "train_iter, valid_iter, test_iter = data.BucketIterator.splits((train, valid, test), \n",
    "                                              batch_sizes=(batch_size, batch_size, batch_size), \n",
    "                                  sort_key=lambda x: len(x.src),\n",
    "                                  shuffle=True,\n",
    "                                  device=DEVICE,\n",
    "                                  sort_within_batch=False)\n",
    "                                  \n",
    "train_iter = BucketIteratorWrapper(train_iter)\n",
    "valid_iter = BucketIteratorWrapper(valid_iter)\n",
    "test_iter = BucketIteratorWrapper(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDGePdsS-IE7"
   },
   "outputs": [],
   "source": [
    "model = make_model(len(SRC.vocab), len(TGT.vocab), N=6, d_model=512, d_ff=2048, h=8, dropout=0.1)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "unoU0-EU-Pm5"
   },
   "outputs": [],
   "source": [
    "pad_idx = TGT.vocab.stoi[\"<pad>\"]\n",
    "criterion = MyCriterion(pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTV-O41H956b"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "2j5_wE81DnKf",
    "outputId": "b4a1a5a8-8e25-486c-86a3-43f967efd83a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 50]) torch.Size([64, 49])\n",
      "torch.Size([64, 49, 28298])\n",
      "torch.Size([64, 49, 28298])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for batch in train_iter:\n",
    "    print(batch.src.shape, batch.tgt_y.shape)\n",
    "    out = model.forward(batch)\n",
    "    print(out.shape)\n",
    "    out = F.log_softmax(out, dim=-1)\n",
    "    print(out.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o-eUsy_m9jyH"
   },
   "outputs": [],
   "source": [
    "def train_epoch(data_iter, model, criterion, optimizer):\n",
    "    total_loss = 0\n",
    "    data_iter = tqdm(data_iter, position=0, leave=True)\n",
    "    counter = 0\n",
    "    for batch in data_iter:\n",
    "        optimizer.zero_grad()\n",
    "        out = model.forward(batch)\n",
    "        out = F.log_softmax(out, dim=-1)\n",
    "        loss = criterion(out, batch.tgt_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        data_iter.set_postfix(loss = loss)\n",
    "        counter +=1\n",
    "    total_loss /= counter\n",
    "    return total_loss\n",
    "\n",
    "def valid_epoch(data_iter, model, criterion):\n",
    "    total_loss = 0\n",
    "    data_iter = tqdm(data_iter)\n",
    "    counter = 0\n",
    "    for batch in data_iter:\n",
    "        \n",
    "        out = model.forward(batch)\n",
    "        out = F.log_softmax(out, dim=-1)\n",
    "        loss = criterion(out, batch.tgt_y)\n",
    "        \n",
    "        total_loss += loss\n",
    "        data_iter.set_postfix(loss = loss)\n",
    "        counter +=1\n",
    "        \n",
    "    total_loss /= counter\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "ZfhR-Iex_OHK",
    "outputId": "be581a7e-faf8-4276-c35a-237860a9e53d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3293/3293 [18:12<00:00,  3.03it/s, loss=tensor(5.3604, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "  0%|          | 1/366 [00:00<00:45,  8.06it/s, loss=tensor(5.2908, device='cuda:0')]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train tensor(5.4829, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 366/366 [00:36<00:00,  9.95it/s, loss=tensor(5.7329, device='cuda:0')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid tensor(5.1875, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1 # try 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loss = train_epoch(train_iter, model, criterion, optimizer)\n",
    "    print('train', loss)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = valid_epoch(valid_iter, model, criterion)\n",
    "        scheduler.step(loss)\n",
    "        print('valid', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bHQzuaNtaZEl"
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "def greedy_search(model, src, src_mask, max_len=10):\n",
    "    start_symbol = TGT.vocab.stoi[\"<s>\"]\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1)).type_as(src.data)), memory, src_mask)\n",
    "        prob = F.log_softmax(out, dim=-1)\n",
    "        _, next_word = torch.max(prob[:, -1], dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t9zlBg4jTYRR"
   },
   "outputs": [],
   "source": [
    "def beam_search(model, src, src_mask, max_len=10, k=5):\n",
    "    start_symbol = TGT.vocab.stoi[\"<s>\"]\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    candidate_sequences = [(ys, 0)]\n",
    "    final_candidates = []\n",
    "    for t in range(max_len):\n",
    "        new_candidate_sequences = []\n",
    "        for i, current_candidate in enumerate(candidate_sequences):\n",
    "            out = model.decode(Variable(current_candidate[0]), \n",
    "                                Variable(subsequent_mask(current_candidate[0].size(1)).type_as(src.data)), memory, src_mask)\n",
    "            prob = F.log_softmax(out, dim=-1)\n",
    "            prob, next_words = torch.topk(prob[:, -1], dim = 1, k = prob.shape[-1], sorted=False)\n",
    "            for j, next_word in enumerate(next_words[0]):\n",
    "                next_word = next_word.item()\n",
    "                new_candidate_sequence = torch.cat([current_candidate[0], torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "                new_candidate_prob = current_candidate[1] + prob[0][j].item()\n",
    "                new_candidate_sequences.append((new_candidate_sequence, new_candidate_prob))\n",
    "            new_candidate_sequences = sorted(new_candidate_sequences, key=lambda x: x[1], reverse=True)[:k]\n",
    "        new_candidate_sequences_ = []\n",
    "        for (seq, prob) in new_candidate_sequences:\n",
    "            if seq[0][-1] == TGT.vocab.stoi['</s>']:\n",
    "                final_candidates.append((seq, prob))\n",
    "                k = k - 1\n",
    "            else:\n",
    "                new_candidate_sequences_.append((seq, prob))\n",
    "        candidate_sequences = new_candidate_sequences_\n",
    "        if k <= 0:\n",
    "            break\n",
    "    final_candidates.extend(candidate_sequences)\n",
    "    return sorted(final_candidates, key=lambda x: x[1], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eDU7vZJBSjKz",
    "outputId": "1a3ca61c-6be8-4137-8b8a-4dd742959c66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2,   4,   9,  33, 410,   3,   3,   3,   3,   3]], device='cuda:0')"
      ]
     },
     "execution_count": 215,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_search(model, src, src_key_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PWQHz5WQX3s6",
    "outputId": "a810372b-1c33-43ce-f0f0-f13ad53dcc4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  2,   4,   9,  33, 410,   3]], device='cuda:0'), -8.522655010223389)"
      ]
     },
     "execution_count": 235,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_search(model, src, src_key_padding_mask, k = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CvURKl0bvUGE",
    "outputId": "9cceb295-0489-4ac9-bb77-df4e61e411b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2, 30,  4, 47,  3]], device='cuda:0'), -9.36168098449707)"
      ]
     },
     "execution_count": 217,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_search(model, src, src_key_padding_mask, k = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "CCGGEk0oIwZZ",
    "outputId": "ac727c4a-4297-4643-9914-6a394db9e626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source: сосуществование\n",
      "Translation:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-147f71bca821>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Translation:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_proba\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(valid_iter):\n",
    "        src = batch.src[:1]\n",
    "        src_key_padding_mask = src != SRC.vocab.stoi[\"<pad>\"]\n",
    "        beam = beam_search(model, src, src_key_padding_mask)\n",
    "        \n",
    "        seq = []\n",
    "        for i in range(1, src.size(1)):\n",
    "            sym = SRC.vocab.itos[src[0, i]]\n",
    "            if sym == \"</s>\": break\n",
    "            seq.append(sym)\n",
    "        seq = tok_ru.decode_pieces(seq)\n",
    "        print(\"\\nSource:\", seq)\n",
    "        \n",
    "        print(\"Translation:\")\n",
    "        for pred, pred_proba in beam:                \n",
    "            seq = []\n",
    "            for i in range(1, pred.size(1)):\n",
    "                sym = TGT.vocab.itos[pred[0, i]]\n",
    "                if sym == \"</s>\": break\n",
    "                seq.append(sym)\n",
    "            seq = tok_en.decode_pieces(seq)\n",
    "            print(f\"pred {pred_proba:.2f}:\", seq)\n",
    "                \n",
    "        seq = []\n",
    "        for i in range(1, batch.tgt.size(1)):\n",
    "            sym = TGT.vocab.itos[batch.tgt[0, i]]\n",
    "            if sym == \"</s>\": break\n",
    "            seq.append(sym)\n",
    "        seq = tok_en.decode_pieces(seq)\n",
    "        print(\"Target:\", seq)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RVTpGY5E9zhU"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8oXUEwp90Zn"
   },
   "outputs": [],
   "source": [
    "hypotheses = []\n",
    "references = []\n",
    "verbose = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "        model.eval()\n",
    "        src = batch.src[:1]\n",
    "        src_key_padding_mask = src != SRC.vocab.stoi[\"<pad>\"]\n",
    "        pred = greedy_search(model, src, src_key_padding_mask)\n",
    "        \n",
    "        seq = []\n",
    "        for i in range(1, src.size(1)):\n",
    "            sym = SRC.vocab.itos[src[0, i]]\n",
    "            if sym == \"</s>\": break\n",
    "            seq.append(sym)\n",
    "        seq = tok_ru.decode_pieces(seq)\n",
    "        if verbose: print(\"\\nSource:\", seq)\n",
    "\n",
    "        seq = []\n",
    "        for i in range(1, pred.size(1)):\n",
    "            sym = TGT.vocab.itos[pred[0, i]]\n",
    "            if sym == \"</s>\": break\n",
    "            seq.append(sym)\n",
    "        seq = tok_en.decode_pieces(seq)\n",
    "        if verbose: print(\"\\nTranslation:\", seq)\n",
    "        hypotheses.append(seq)\n",
    "                \n",
    "        seq = []\n",
    "        for i in range(1, batch.tgt.size(1)):\n",
    "            sym = TGT.vocab.itos[batch.tgt[0, i]]\n",
    "            if sym == \"</s>\": break\n",
    "            seq.append(sym)\n",
    "        seq = tok_en.decode_pieces(seq)\n",
    "        if verbose: print(\"Target:\", seq)\n",
    "        references.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "znJYDj3M90xR",
    "outputId": "505018c1-1356-4a28-f6cb-e0be31364d62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0034547118824337967"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bleu(references, hypotheses, \n",
    "            smoothing_function=SmoothingFunction().method3,\n",
    "            auto_reweigh=True\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mPo9wWXkVD0_"
   },
   "outputs": [],
   "source": [
    "hypotheses = []\n",
    "references = []\n",
    "verbose = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "        model.eval()\n",
    "        src = batch.src[:1]\n",
    "        src_key_padding_mask = src != SRC.vocab.stoi[\"<pad>\"]\n",
    "        pred, proba = beam_search(model, src, src_key_padding_mask)\n",
    "        \n",
    "        seq = []\n",
    "        for i in range(1, src.size(1)):\n",
    "            sym = SRC.vocab.itos[src[0, i]]\n",
    "            if sym == \"</s>\": break\n",
    "            seq.append(sym)\n",
    "        seq = tok_ru.decode_pieces(seq)\n",
    "        if verbose: print(\"\\nSource:\", seq)\n",
    "\n",
    "        seq = []\n",
    "        for i in range(1, pred.size(1)):\n",
    "            sym = TGT.vocab.itos[pred[0, i]]\n",
    "            if sym == \"</s>\": break\n",
    "            seq.append(sym)\n",
    "        seq = tok_en.decode_pieces(seq)\n",
    "        if verbose: print(\"\\nTranslation {} prob {}:\".format(seq, proba))\n",
    "        hypotheses.append(seq)\n",
    "                \n",
    "        seq = []\n",
    "        for i in range(1, batch.tgt.size(1)):\n",
    "            sym = TGT.vocab.itos[batch.tgt[0, i]]\n",
    "            if sym == \"</s>\": break\n",
    "            seq.append(sym)\n",
    "        seq = tok_en.decode_pieces(seq)\n",
    "        if verbose: print(\"Target:\", seq)\n",
    "        references.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0OJv3X1dVWml",
    "outputId": "33fb57fb-3166-4335-e201-dd9773e10486"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007427678827096992"
      ]
     },
     "execution_count": 119,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bleu(references, hypotheses, \n",
    "            smoothing_function=SmoothingFunction().method3,\n",
    "            auto_reweigh=True\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kAYOCIyXHWZ-"
   },
   "source": [
    "<div id=\"disqus_thread\"></div>\n",
    "<script>\n",
    "    /**\n",
    "     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.\n",
    "     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables\n",
    "     */\n",
    "    /*\n",
    "    var disqus_config = function () {\n",
    "        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable\n",
    "        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable\n",
    "    };\n",
    "    */\n",
    "    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW\n",
    "        var d = document, s = d.createElement('script');\n",
    "        \n",
    "        s.src = 'https://EXAMPLE.disqus.com/embed.js';  // IMPORTANT: Replace EXAMPLE with your forum shortname!\n",
    "        \n",
    "        s.setAttribute('data-timestamp', +new Date());\n",
    "        (d.head || d.body).appendChild(s);\n",
    "    })();\n",
    "</script>\n",
    "<noscript>Please enable JavaScript to view the <a href=\"https://disqus.com/?ref_noscript\" rel=\"nofollow\">comments powered by Disqus.</a></noscript>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": " The Annotated Transformer -  assignment code + data - 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
