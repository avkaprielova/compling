{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oJiebI3eDOL",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S75dEHP-_Rm8",
        "colab_type": "text"
      },
      "source": [
        "Develop a model for 20 news groups dataset from scikit-learn. Select 20% of data for test set.\n",
        "\n",
        "Develop metric learning model with siamese network [3 points] and softmax loss [3 points] (from seminar). Use KNN and LSH (any library for approximate nearest neighbor search) for final prediction after the network was trained. [2 points]\n",
        "\n",
        "! Remember, that LSH gives you a set of neighbor candidates, for which you have to calculate distances to choose top-k nearest neighbors.\n",
        "\n",
        "Your quality metric = accuracy score [2 points if acc > 0.8 ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgUj9RghENFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPzkqYt2EuJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(newsgroups_train.data), len(newsgroups_train.data), len(newsgroups_train.target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiUqxUSwGHkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "newsgroups_train.target_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKRZP2DWFaYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_vals = set(newsgroups_train.target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5AIewQJFSzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = {x: [] for x in set(newsgroups_train.target)}\n",
        "for idx, anchor in enumerate(newsgroups_train.data):\n",
        "    target = newsgroups_train.target[idx]\n",
        "    df[target] = df[target] + [anchor]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWG6B5xpFoR0",
        "colab_type": "code",
        "outputId": "c4c7d3f2-d117-4355-c1a4-35d0e258facc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(df[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "480"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S7nz5oHG2kz",
        "colab_type": "code",
        "outputId": "412855ad-7dff-498c-dbdc-7051da67e266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(target_vals)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "set"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuxY-a0lGVG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdlV9eQV0zM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# form new dataset:\n",
        "#    1) anchor: новостная статья\n",
        "#    2) pos: случайная другая новостная статья из той же группы\n",
        "#    3) neg: случайная другая новостная статья из любой другой группы"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt3ygdqLDEeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_df = {}\n",
        "for idx, anchor in enumerate(newsgroups_train.data):\n",
        "    target = newsgroups_train.target[idx]\n",
        "    pos = random.choice(df[target])\n",
        "    another_group = random.choice(list(target_vals - set([target])))\n",
        "    neg = random.choice(df[another_group])\n",
        "    new_df[idx] = {'anchor': anchor, 'pos': pos, 'neg': neg, 'target': target}\n",
        "new_df = pd.DataFrame.from_dict(new_df, orient = 'index')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axBQl_NtHeBM",
        "colab_type": "code",
        "outputId": "b3ed68e4-ce40-4ad1-ef65-9f349da0c2d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "new_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>anchor</th>\n",
              "      <th>pos</th>\n",
              "      <th>neg</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
              "      <td>From: rhorwell@crab.atc.boeing.com (Roland Far...</td>\n",
              "      <td>From: valo@cvtstu.cvt.stuba.cs (Valo Roman)\\nS...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
              "      <td>From: Thomas Kephart &lt;kephart@snowhite.eeap.cw...</td>\n",
              "      <td>From: pmoloney@maths.tcd.ie (Paul Moloney)\\nSu...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
              "      <td>From: pritchet@cs.scarolina.edu (Ronald W. Pri...</td>\n",
              "      <td>From: hal@cco.caltech.edu (Hal Finney)\\nSubjec...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
              "      <td>From: ukrphil@prlhp1.prl.philips.co.uk (M.J.Ph...</td>\n",
              "      <td>From: brandt@cs.unc.edu (Andrew Brandt)\\nSubje...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
              "      <td>Subject: Diffs to sci.space/sci.astro Frequent...</td>\n",
              "      <td>From: rmohns@vax.clarku.edu\\nSubject: RE: Can ...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              anchor  ... target\n",
              "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...  ...      7\n",
              "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...  ...      4\n",
              "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...  ...      4\n",
              "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...  ...      1\n",
              "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...  ...     14\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYCuC6V3048Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute two different representation for each token.\n",
        "# Each representation is a linear weighted combination for the\n",
        "# 3 layers in ELMo (i.e., charcnn, the outputs of the two BiLSTM))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEvOfVHP_QeT",
        "colab_type": "code",
        "outputId": "ee63d0ef-eee9-4c82-ca04-c5090e6b611f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install allennlp\n",
        "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
        "\n",
        "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
        "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
        "\n",
        "elmo = Elmo(options_file, weight_file, 2, dropout=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Collecting spacy<2.2,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/5b/e07dd3bf104237bce4b398558b104c8e500333d6f30eabe3fa9685356b7d/spacy-2.1.9-cp36-cp36m-manylinux1_x86_64.whl (30.8MB)\n",
            "\u001b[K     |████████████████████████████████| 30.9MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: gevent>=1.3.6 in /tensorflow-1.15.0/python3.6 (from allennlp) (1.4.0)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/b8/a8588d4010f13716a324f55d23999259bad9db2320f4fe919a66b2f651f3/jsonnet-0.15.0.tar.gz (255kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 60.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Collecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/72/dd/ac49f9c69540d7e09210415801a05d0a54d4d0ca8401503c46847dacd3a0/overrides-2.8.0.tar.gz\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.12.23)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d8/5e877ac5e827eaa41a7ea8c0dc1d3042e05d7e337604dc2aedb854e7b500/ftfy-5.7.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.0MB/s \n",
            "\u001b[?25hCollecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 60.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Collecting numpydoc>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/70/4d8c3f9f6783a57ac9cc7a076e5610c0cc4a96af543cafc9247ac307fbfe/numpydoc-0.9.2.tar.gz\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.18.2)\n",
            "Collecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.1)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 63.2MB/s \n",
            "\u001b[?25hCollecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/6b/fbb2d499b96861a18c1641f6fefe775110d3faba65c1524950e9ad64824a/jsonpickle-1.3-py2.py3-none-any.whl\n",
            "Collecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Collecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Collecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.0MB/s \n",
            "\u001b[?25hCollecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 61.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 66.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.0)\n",
            "Collecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/a5/52/8063322bd9ee6e7921b74fcb730c6ba983ff995ddfabd966bb689e313464/responses-0.10.12-py2.py3-none-any.whl\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.6.0)\n",
            "Collecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 13.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 61.3MB/s \n",
            "\u001b[?25hCollecting thinc<7.1.0,>=7.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 45.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /tensorflow-1.15.0/python3.6 (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->allennlp) (1.12.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.23 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.15.23)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.3.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.8)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 50.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (2.11.1)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.0.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.1.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.2.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (46.0.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.6)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->allennlp) (0.15.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask>=1.0.2->allennlp) (1.1.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (20.3)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.8.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n",
            "Building wheels for collected packages: jsonnet, overrides, ftfy, numpydoc, word2number, parsimonious\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.15.0-cp36-cp36m-linux_x86_64.whl size=3319882 sha256=d7674147d2b45109853b3d68ea083651aeaa327bfb4ebde395bdbc428fb4d7e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/63/2e/da89cfe1ba08550bd7262d5d9c027edc313980c3b85b3b0a38\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.8.0-cp36-none-any.whl size=5609 sha256=928cbfab48900bbfb8b445df12afc51138b84f35a48ca3401cf43264283fdd85\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/f1/ba/eaf6cd7d284d2f257dc71436ce72d25fd3be5a5813a37794ab\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.7-cp36-none-any.whl size=44593 sha256=d7ad3d068569e50ba093d51572fbeec80a39fc2364209f36b0a25e637c6c8790\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/da/59/6c8925d571aacade638a0f515960c21c0887af1bfe31908fbf\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpydoc: filename=numpydoc-0.9.2-cp36-none-any.whl size=31893 sha256=8dc017b0639e13d541264c51bac8ee32c569f670616454b65f850c6fa95ffac6\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/f3/52/25c8e1f40637661d27feebc61dae16b84c7cdd93b8bc3d7486\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5587 sha256=d8d16878245f9e15b646c716dd3895f9356874e3b2c10f0765af759306565c84\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42712 sha256=0128d3df6f50d36ffad2ca9e7f54d14bd7fb684539a544a769e40c83cfbe3601\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "Successfully built jsonnet overrides ftfy numpydoc word2number parsimonious\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: plac, preshed, blis, thinc, spacy, jsonnet, overrides, ftfy, sentencepiece, pytorch-transformers, numpydoc, flaky, flask-cors, unidecode, jsonpickle, conllu, word2number, parsimonious, pytorch-pretrained-bert, tensorboardX, responses, allennlp\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: preshed 3.0.2\n",
            "    Uninstalling preshed-3.0.2:\n",
            "      Successfully uninstalled preshed-3.0.2\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed allennlp-0.9.0 blis-0.2.4 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.7 jsonnet-0.15.0 jsonpickle-1.3 numpydoc-0.9.2 overrides-2.8.0 parsimonious-0.8.1 plac-0.9.6 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.10.12 sentencepiece-0.1.85 spacy-2.1.9 tensorboardX-2.0 thinc-7.0.8 unidecode-1.1.1 word2number-1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 336/336 [00:00<00:00, 945195.27B/s]\n",
            "100%|██████████| 374434792/374434792 [00:23<00:00, 16144482.16B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Nu1jzhs08ka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# преобразовать pandas dataframe в pytorch dataset для цикла по batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGZB11ifinh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_train, df_val = train_test_split(new_df, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "xa_train = batch_to_ids(df_train.anchor.values)\n",
        "xp_train = batch_to_ids(df_train.pos.values)\n",
        "xn_train = batch_to_ids(df_train.neg.values)\n",
        "y_train = tt.from_numpy(df_train.target.values).float()\n",
        "\n",
        "xa_val = batch_to_ids(df_val.anchor.values)\n",
        "xp_val = batch_to_ids(df_val.pos.values)\n",
        "xn_val = batch_to_ids(df_val.neg.values)\n",
        "y_val = tt.from_numpy(df_val.is_duplicate.values).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlYNLoGSij8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "batch_size = 32 # try 16\n",
        "train_loader = DataLoader(TensorDataset(xa_train, xp_train, xn_train, y_train), batch_size=batch_size)\n",
        "val_loader = DataLoader(TensorDataset(xa_val, xp_val, xn_val, y_val), batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWxJff4G9Mhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _train_epoch(model, iterator, optimizer, curr_epoch):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0\n",
        "\n",
        "    n_batches = len(iterator)\n",
        "    iterator = tqdm_notebook(iterator, total=n_batches, desc='epoch %d' % (curr_epoch), leave=True)\n",
        "\n",
        "    for i, batch in enumerate(iterator):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = model(batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        curr_loss = loss.data.cpu().detach().item()\n",
        "        \n",
        "        loss_smoothing = i / (i+1)\n",
        "        running_loss = loss_smoothing * running_loss + (1 - loss_smoothing) * curr_loss\n",
        "\n",
        "        iterator.set_postfix(loss='%.5f' % running_loss)\n",
        "\n",
        "    return running_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo1jGo_m9W5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _test_epoch(model, iterator):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    n_batches = len(iterator)\n",
        "    with tt.no_grad():\n",
        "        for batch in iterator:\n",
        "            loss = model(batch)\n",
        "            epoch_loss += loss.data.item()\n",
        "\n",
        "    return epoch_loss / n_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdeLtS0I9atO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nn_train(model, train_iterator, valid_iterator, optimizer, n_epochs=100,\n",
        "          scheduler=None, early_stopping=0):\n",
        "\n",
        "    prev_loss = 100500\n",
        "    es_epochs = 0\n",
        "    best_epoch = None\n",
        "    history = pd.DataFrame()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss = _train_epoch(model, train_iterator, optimizer, epoch)\n",
        "        valid_loss = _test_epoch(model, valid_iterator)\n",
        "\n",
        "        valid_loss = valid_loss\n",
        "        print('validation loss %.5f' % valid_loss)\n",
        "\n",
        "        record = {'epoch': epoch, 'train_loss': train_loss, 'valid_loss': valid_loss}\n",
        "        history = history.append(record, ignore_index=True)\n",
        "\n",
        "        if early_stopping > 0:\n",
        "            if valid_loss > prev_loss:\n",
        "                es_epochs += 1\n",
        "            else:\n",
        "                es_epochs = 0\n",
        "\n",
        "            if es_epochs >= early_stopping:\n",
        "                best_epoch = history[history.valid_loss == history.valid_loss.min()].iloc[0]\n",
        "                print('Early stopping! best epoch: %d val %.5f' % (best_epoch['epoch'], best_epoch['valid_loss']))\n",
        "                break\n",
        "\n",
        "            prev_loss = min(prev_loss, valid_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVdLzbe-L33F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Tripletnet(elmo)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "nn_train(model, train_loader, val_loader, optimizer, n_epochs=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwE_gcW5ifkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def triplet_loss(anchor_embed, pos_embed, neg_embed):\n",
        "    return F.cosine_similarity(anchor_embed, neg_embed) - F.cosine_similarity(anchor_embed, pos_embed)\n",
        "    \n",
        "    \n",
        "class Tripletnet(nn.Module):\n",
        "    def __init__(self, elmo, fc_num = 1024, hidden_dim = 128):\n",
        "        super(Tripletnet, self).__init__()\n",
        "        self.elmo = elmo\n",
        "        self.criterion = criterion\n",
        "        \n",
        "        self.fc = nn.Linear(fc_num*3, hidden_dim)\n",
        "        \n",
        "    def branch(self, x):\n",
        "        x = self.elmo(x)['elmo_representations'] # vector representation\n",
        "        x = tt.cat(x, dim=-1)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, batch):\n",
        "        anchor, pos, neg = batch\n",
        "        anchor = self.branch(anchor)\n",
        "        pos = self.branch(pos)\n",
        "        neg = self.branch(neg)\n",
        "        \n",
        "        return triplet_loss(anchor, pos, neg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BN-8gqQJLj6",
        "colab_type": "text"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf55WOapJK3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "neigh = KNeighborsClassifier(n_neighbors=1)\n",
        "neigh.fit(anchor, newsgroups_train.target)\n",
        "neight.predict(anchor_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEmKNC4kJNX5",
        "colab_type": "text"
      },
      "source": [
        "# LSH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBVMZQQ5JNg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import LSHForest\n",
        "\n",
        "lshf = LSHForest(random_state=42)\n",
        "lshf.fit(train_loader)\n",
        "LSHForest(min_hash_match=4, n_candidates=50, n_estimators=1,\n",
        "          n_neighbors=5, radius=1.0, radius_cutoff_ratio=0.9,\n",
        "          random_state=42)\n",
        "distances, indices = lshf.kneighbors(X_test, n_neighbors=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK7A57XgJWEa",
        "colab_type": "text"
      },
      "source": [
        "# accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A12G7Kl8iG9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grid-search over hidden_dim and fc_num"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}